{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necesseties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Embedding, Dense, LSTM \n",
    "from keras.models import Sequential \n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/review_seperated_by_sentences_conclusion.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make text lowercase, remove text in square brackets, remove links, remove HTML tags,\n",
    "remove punctuation, remove words containing numbers, remove all single characters, \n",
    "and substitute multiple spaces with single space.\n",
    "'''\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "data['sentence'] = data['sentence'].apply(lambda x:clean_text(x))\n",
    "\n",
    "# delete row with missing values\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x_temp'] = data['sentence'].apply(lambda x:str(x).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in data['x_temp'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(x):   \n",
    "    return [y for y in x if y not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['x_temp'] = data['x_temp'].apply(lambda x:remove_stopword(x))\n",
    "top = Counter([item for sublist in data['x_temp'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    if x == 0 or x == '0': # neutral or unknown\n",
    "        x = [1, 0, 0]\n",
    "    elif x == 1 or x == '1': # positive\n",
    "        x = [0, 1, 0]\n",
    "    elif x == 2 or x == '2': # negative\n",
    "        x = [0, 0, 1]\n",
    "    else:\n",
    "        x = None\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_temp'] = data['sentiment'].apply(lambda x:one_hot(x))\n",
    "\n",
    "# delete row with poorly classified sentiment\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, train_size = 0.80, random_state = 0)\n",
    "\n",
    "x_train = train_data['x_temp']\n",
    "x_test = test_data['x_temp']\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(train_data['y_temp'])):\n",
    "    y_train.append(train_data['y_temp'].iloc[i])\n",
    "    \n",
    "for i in range(len(test_data['y_temp'])):\n",
    "    y_test.append(test_data['y_temp'].iloc[i])\n",
    "    \n",
    "y_train = np.array(y_train) \n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 35000 \n",
    "tokenizer = Tokenizer(num_words = max_words) \n",
    "tokenizer.fit_on_texts(x_train) \n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_len = max(len(l) for l in x_train)\n",
    "train_average_len = sum(map(len, x_train))/ len(x_train)\n",
    "test_max_len = max(len(l) for l in x_test)\n",
    "test_average_len = sum(map(len, x_test))/ len(x_test)\n",
    "\n",
    "print(\"=====Train Data=====\")\n",
    "print(\"max length: \", train_max_len) \n",
    "print(\"average length: \", train_average_len)\n",
    "print(\"\\n=====Test Data=====\")\n",
    "print(\"max length: \", test_max_len) \n",
    "print(\"average length: \", test_average_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(train_max_len, test_max_len)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_len) \n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100)) \n",
    "model.add(LSTM(128)) \n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "history = model.fit(x_train, y_train, batch_size=10, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy: {:.2f}%\".format(model.evaluate(x_test, y_test)[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
